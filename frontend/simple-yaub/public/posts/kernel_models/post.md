# Kernel Methods

A previous blog post on Linear Models explained a
beautiful connection between linear models and kernels. It has also set
the stage for us to go progress towards what more commonly referred to
as Kernel Methods. Probabilistic techniques that utilise kernel
functions to model complex behaviours in the data.

## Gaussian Processes for Regression

In Linear models, we saw a bunch of linear models that
assumed some functional form. Most of them, if not all, can be
represented by some (linear) combination of the targets. Now, consider
that instead of finding optimal function parameters, we would try to
find functions (function mappings). This way, we can directly sample a
function from a governing (multivariate) distribution. How could one
parameterise a distribution over functions? Well, do not parameterise
functions directly, but model the joint marginal probabilities of their
values. We then reason about the entire function space by evaluating the
joint probabilities at any set of input points.

Let $X = \{\vec{x}_1, \vec{x}_2 ... \vec{x}_N\}$ be a finite set of
elements and consider a set $\mathcal{H}$ of all possible functions
$f \mapsto t$ mapping from $\mathcal{X}$ to $\mathbb{R}^{N}$. If we
assume that the domain of any $f(\cdot) \in \mathcal{H}$ has a limited
($N$) number of elements, we can say that $f$ is "represented" by its
vector $\vec{f} = [f(\vec{x}_1), f(\vec{x}_2)... f(\vec{x}_N)]$. Note
that $f(\vec{x}_i)$'s by themselves can be seen as random
variables/functions if the underlying function $f(\cdot)$ is also
random. Thus, there is a one-to-one correspondence between a function
$f(\cdot) \in \mathcal{H}$ and its "vector form\" representation.
Finally, we can specify a governing probability distribution such that
$\vec{f} \sim N(\vec{\mu}, \Lambda)$. This implies a (joint) probability
distribution over random variables/functions that could have produced
those values, i.e., $p(\vec{f}|X) = N(\vec{\mu}, K))$. Hence, the
probability distribution for the underlying function
$f(\cdot) \in \mathcal{H}$.

In most of the cases, we need to assume that our stream of data is
infinite and we can always make new predictions based on the new
observations $\vec{x}^{*}$ , i.e., a continuing process. Gaussian
process (GP) is a stochastic process - collection of random variables -
such that any finite sub-collection of random variables from the process
has a joint multivariate distribution. By assuming a Gaussian process,
we are pretty much saying: there were these values $\vec{f}$ from
underlying $f$, now any new collection $\vec{f}^{*}$ must come from the
same $f$, i.e., random variables, elements of $\vec{f}^{*}$, come from
the same joint multivariate distribution.

Assume a GP is defined as $\vec{f} \sim N(\vec{0}, K(\cdot, \cdot))$.
The function that produces $\vec{f}$ or $\vec{f}^{*}$ is zero-meaned and
generates values according to the kernel function $K(\cdot, \cdot)$.
Just like before the kernel identifies how we are going to take
neighbours of a new observation and mix/combine/weigh their target
values to arrive to the prediction for the new observation. Generally,
if two points are similar in the kernel space, the function values at
these points will also be also similar.

Lastly, we look of how to make prediction with GP. Imagine, we have
collected a training set $S=\{\vec{x_i}, t_i \}_{i=1}^N$ of i.i.d.
examples. If we zero-mean the sample's observations - GP was assumed
with the mean 0 - we can apply our process to the data and say
$t_i=f(\vec{x_i}) + \epsilon_{i}$. This means the observed targets $t_i$
are generated by a noisy process where the underlying function values
follow a Gaussian process. Now, let
$S^{*}=\{\vec{x_i}^{*}, t_{i}^{*} \}_{i=1}^{N^{*}}$ be a test set of
points that come from the same process, i.e.,
$t_{i}^{*}=f(\vec{x_i}^{*}) + \epsilon_{i}^{*}$. Since, we said said
that any sub-collection of random variables drawn from a GP has a joint
multivariate distribution, then we can define the multivariate Gaussian
as 

$$
\begin{align}
\begin{bmatrix}
\vec{f} \\ 
\vec{f}^*
\end{bmatrix} 
\bigg\rvert\, X, X^{*} 
&\sim \mathcal{N}\left(\vec{0},
\begin{bmatrix}
K(X, X) & K\left(X, X^{*}\right) \\ 
K\left(X^*, X\right) & K\left(X^*, X^*\right)
\end{bmatrix}\right) 
\end{align}
$$
[//]: # (\label{eq:gp_0})

Additionally, there is white noise i.i.d. noise that is natural to all
the real-world data 

$$
\begin{align} 
\left[\begin{array}{l}\vec{\epsilon} \\ \vec{\epsilon}^*\end{array}\right] &\sim \mathcal{N}\left(\vec{0},\left[\begin{array}{cc} 
\sigma^2 \mathbb{1} & \vec{0} \\ 
\vec{0} & \sigma^2 \mathbb{1}
\end{array}\right]\right)
\end{align}
$$
[//]: # (\label{eq:gp_1})

This gives us function values with some white noise around them. Since
the white noise is i.i.d. we can still use GP frame, but instead of
figuring out estimates for $\vec{f}^{*}$, we are searching for
$\vec{t}^{*}$. 

$$
\begin{align} 
\left[\begin{array}{l}\vec{t}_i \\ \vec{t}_i^*\end{array}\right]_{|X,X^{*}} = 
\begin{bmatrix}
\vec{f} \\ 
\vec{f}^*
\end{bmatrix}  +
\left[\begin{array}{l}\vec{\varepsilon} \\ \vec{\varepsilon}\end{array}\right] \sim \mathcal{N}\left(\vec{0},\left[\begin{array}{cc}K(X, X)+\sigma^2\mathbb{1} & K(X, X^{*}) \\ K\left(X^*, X\right) & \left.K\left(X, X^*\right)+\sigma^2\mathbb{1}\right]\end{array}\right]\right. \ . 
\end{align}
$$
[//]: # (\label{eq:gp_2})

If we were to take the rules of [conditional distributions of the
multivariate
Gaussian's](#https://statproofbook.github.io/P/mvn-cond.html), we can
find mean and covariance for the conditional distribution of
$p(\vec{t}^* \mid \vec{t}, X, X^*) = N\left(\vec{\mu}^*, \Sigma^*\right)$

$$
\begin{align} 
\vec{\mu}^{*} &= K\left(X^*, X\right)\left(K(X, X)+\sigma^2 \mathbb{1}\right)^{-1} \vec{t} \\ 
\Sigma^* &= K\left(X^* , X^*\right) + \sigma^2 \mathbb{1} - K\left(X^* , X\right) - \left(K(X, X) + \sigma^2 \mathbb{1} \right)^{-1} K\left(X, X^*\right)
\end{align}
$$
[//]: # (\label{eq:gp_3})

There are two things to keep in mind when using GP in the wild. First,
look how compute and memory expensive it is to make a new prediction..
If we have a lot of past data, computing the kernels and inverting those
each time a new observation comes can be extremely inefficient. That is
why the real-world implementations of GP's usually do not look at the
whole covariance matrix of observations, i.e., evaluating the kernel
function for all data points, instead they use a sample of the data for
each new set of observations. Those optimised methods are often called
sparse kernel methods or sparse kernel machines. Second, the choice of
kernels is another aspect to consider. Many kernel function comes with
its own behaviour and hyperparameters. As a result, a practitioner can
spend a considerable amount of time in choosing the right kernel and
optimising its hyperparameters.

## Gaussian Processes for Classification

The solution for GP's in the case of a regression task, though
computationally heavy, is analytical. If we put GP's into the framework
of classification tasks, we end up with an even less behaved solution.

Now, we are predicting a variable that can only take either of two value
$t_i \in\{0 ; 1\}$. Use a linear separator function $a(\cdot)$ and put
it into a sigmoid to map a value to the probability range of $[0, 1]$

$$
\begin{align} 
a(\vec{x}) : \vec{x} \in \mathbb{R}^M \mapsto a \in \mathbb{R}^1 \\ 
\sigma(a) = \frac{1}{1 + e^{-a}} \\ 
\sigma(a): a \in \mathbb{R}^1 \mapsto [0,1] \ .
\end{align}
$$
[//]: # (\label{eq:gp_class_0})

Then, the one define the probability for the target to be the Bernoulli
distribution 

$$
\begin{align} 
p(t \mid a)=\sigma(a)^t (1-\sigma(a))^{1-t} . \ 
\end{align}
$$
[//]: # (\label{eq:gp_class_1_a})

Unlike the regression case, variable $t$ is defined with a non-Gaussian
distribution. This prevents us from entering the GP framework right away
to model the target. However, the values produced by $a(\cdot)$ are
continuous and can be modeled with a Gaussian process just like in the
regression case. Consequently, we can define a zero-meaned prior with
the covariance function produced by a chosen kernel function

$$
\begin{align} 
p(\vec{a}) = N(\vec{a} | \vec{0}, C_N) \ .
\end{align}
$$
[//]: # (\label{eq:gp_class_1_b})

The final predictive probability distribution, the probability of
$\vec{t}^*$ given $\vec{t}, X, \vec{x}^{*}$, is predictably

$$
\begin{align} 
p(t^* \mid \vec{t}, X, \vec{x}^{*}) \text{\ or, simply, \ \ } p(t^* \mid \vec{t}) = ?
\end{align}
$$
[//]: # (\label{eq:gp_class_2})

We can rewrite it as 

$$
\begin{align} 
p\left(t^* \mid \vec{t}\right) = \int p\left(t^* \mid a^{*}\right) p\left(a^{*} | \vec{t}\right) d a^{*}
\end{align}
$$
[//]: # (\label{eq:gp_class_3})

where
$p\left(t^* \mid a^{*}\right) = p\left(t^* = 1 | a^{*}\right) = \sigma\left(a^{*}\right)$.
We see that the first term $p\left(t^* \mid a^{*}\right)$ is a sigmoid
and the posterior $p\left(a^{*} | \vec{t}\right)$ can be rewritten in
the following form 

$$
\begin{align} 
p\left(a_{*} \mid \vec{t} \right) &= \int p\left(a^{*}, \vec{a} \mid \vec{t} \right) d \vec{a} = \\
& = \int \frac{ p\left(\vec{t} \left(a^{*}, \vec{a} \right) \cdot p\left(a^{*}, \vec{a} \right)\right.} {p\left(\vec{t} \right)} d \vec{a} = \\
& = \frac{1}{p (\vec{t} )}  \int p\left(a^{*}, \vec{a} \right) p\left(\vec{t} \mid a^{*}, \vec{a} \right) d \vec{a} = \\ 
& = \left\{P\left(\vec{t} \mid a^{*}, \vec{a} \right) = p\left(\vec{t} \left(\vec{a} \right)\right) \text{\ since $\vec{t} $ is independent of } a^{*} \right\} = \\ 
& = \frac{1}{p (\vec{t} )} \int p\left(a^{*} \mid \vec{a}\right) p\left(\vec{a} \right) p\left(\vec{t} | \vec{a} \right) d \vec{a} \\
& = \int p\left(a^{*} | \vec{a} \right) p\left(\vec{a} \mid \vec{t} \right) d \vec{a} \ .
\end{align}
$$
[//]: # (\label{eq:gp_class_4})

The integral $p\left(t^* \mid \vec{t}\right)$ can be approximate with multiple methods, see
s√®nior's [Bishop Section
6.4.5](#https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
to get an idea of those methods. He himself - maybe seeking for an
easier to write solution - goes for the Laplace approximation method
since $p\left(a_{*} \mid \vec{t} \right)$ can be so neatly reconstructed. The term
$p\left(a^{*} | \vec{a} \right)$ as already mentioned is the prior and
can be found by using the GP for regressions. What is left is to notice
that $p\left(a_{*} \mid \vec{t} \right)$ gives us a convolution of two Gaussian's if
we were two approximate $p\left(\vec{a} \mid \vec{t} \right)$ as such.
Then, one can the solution for the posterior and predictive distribution
by using [the
formula](#https://math.stackexchange.com/questions/1745174/convolution-of-two-gaussian-functions)
for convolving two Gaussian's.


<br>
<br>

<style>
  .intermezzo1 {
    border: none;
    border-top: 2px solid #ccc;
    margin: 2px 0;
    width: 100%; /* or 100% if you want it to span the full width */
    text-align: center;
  }
  .intermezzo1::before {
    content: "‚òÖ Intuition: Convolutions ‚òÖ"; 
    display: inline-block;
    position: relative;
    top: -14px;
    background: white;
    padding: 10px 5px;
    color: #4e3737;
    font-size: 1.2em;
  }
</style>
<hr class="intermezzo1">

The general formula for a convolution is:

$$
(f * g)(x) = \int_{-\infty}^{\infty} f(t) g(x - t) \, dt
\tag{I.1}
$$

We can see $p\left(\vec{a} \mid \vec{t} \right)$ as $ f(t) $ because a zero mean was assumed for the $ a(\cdot) $ 's GP. Thus, the formula in the exponent of $p\left(\vec{a} \mid \vec{t} \right)$ is something like:

$$
-\frac{1}{(2\pi \sigma^2)^{1/2}} a({\vec{x}})^2 \tag{I.2}
$$

Similarly, $ p\left(a^{*} \mid \vec{a} \right) $ corresponds to $ g(x - t) $, since conditioning on $ \vec{a} $ will lead to something along the lines of:

$$
-\frac{1}{(2\pi \sigma^2)^{1/2}} \left( a(\vec{x}^{*}) - a(\vec{x}) \right)^2  \tag{I.3}
$$

Very roughly speaking, you can interpret this convolution as marginalizing out $\vec{a}$ and determining the probability of $ a^{*} $.

<style>
  .intermezzo12 {
    border: none;
    border-top: 2px solid #ccc;
    margin: 40px 0;
    width: 100%; /* or 100% if you want it to span the full width */
    text-align: center;
  }
  .intermezzo12::before {
    display: inline-block;
    position: relative;
    top: -14px;
    background: white;
    padding: 10px 10px;
    color: #666;
    font-size: 1.2em;
  }
</style>

<br>

[//]: # (<hr class="intermezzo12">)


<style>
  .intermezzo21 {
    border: none;
    border-top: 2px solid #ccc;
    margin: 2px 0;
    width: 100%; /* or 100% if you want it to span the full width */
    text-align: center;
  }
  .intermezzo21::before {
    content: "‚òÖ Intuition: Laplace approximation ‚òÖ"; 
    display: inline-block;
    position: relative;
    top: -14px;
    background: white;
    padding: 10px 5px;
    color: #4e3737;
    font-size: 1.2em;
  }
</style>
<hr class="intermezzo21">

Laplace's approximation takes a uni-modal function where most of the probability mass is concentrated around its mode and approximates it with a Normal density function.

How do we know that $p\left(\vec{a}_N \mid \vec{t}_N\right)$ is a function that can be approximated with the Laplace method? We can see that

$$
p(\vec{a}|\vec{t}) \propto p(\vec{a}) \cdot p(\vec{t} | \vec{a}) \text{ where}
$$

$$
p(\vec{a}) = N(\vec{a} | \vec{0}, {C}_N) \text{ , and }
$$

$$
p\left(\vec{t} \mid \vec{a} \right) = \prod_{n=1}^N \sigma\left(a_n\right)^{t_n}\left(1-\sigma\left(a_n\right)\right)^{1-t_n}=\prod_{n=1}^N e^{a_n t_n} \sigma\left(-a_n\right).
$$

The first term, $p(\vec{a}_{N})$, is a normal distribution by (previous) construction, and therefore uni-modal. The second term, $p\left(\vec{t}_N \mid \vec{a}_N\right)$, is a Bernoulli variable, and multiplication of a Bernoulli and a Gaussian is another (possibly skewed) distribution with a single mode (see [here](https://math.stackexchange.com/questions/3315169/what-is-the-distribution-of-the-product-of-a-bernoulli-0-1-a-normal-random-v), [here](https://stats.stackexchange.com/questions/27097/what-is-the-distribution-of-the-product-of-a-bernoulli-a-normal-random-variabl), and [here](https://www.math.wm.edu/~leemis/chart/UDR/PDFs/BernoulliP.pdf)). Bishop provides more details on why this is the case, and we can assume that $p(\vec{a}_N|\vec{t}_N)$ could be approximated by the Laplace method.

The approximation itself is a bit too cumbersome to go into detail. The idea is that one needs to first find the mode of $p(\vec{a} |\vec{t})$ by Taylor expanding the logarithm of $p(\vec{a} | \vec{t})$ and setting its derivative to zero. Second, the Hessian matrix helps to find the covariance matrix of the approximate distribution. This is because the inverse of the Hessian matrix of the log-likelihood function at the maximum likelihood estimate is asymptotically equal to the covariance matrix of the parameter estimates (see [Gaussian curvatures](https://en.wikipedia.org/wiki/Gaussian_curvature) and [Fisher information matrix](https://stats.stackexchange.com/questions/68080/basic-question-about-fisher-information-matrix-and-relationship-to-hessian-and-s)).


<style>
  .intermezzo22 {
    border: none;
    border-top: 2px solid #ccc;
    margin: 40px 0;
    width: 100%; /* or 100% if you want it to span the full width */
    text-align: center;
  }
  .intermezzo22::before {
    display: inline-block;
    position: relative;
    top: -14px;
    background: white;
    padding: 10px 10px;
    color: #666;
    font-size: 1.2em;
  }
</style>
<hr class="intermezzo22">


Say, the Laplace approximation yields 

$$
\begin{align} 
q(\vec{a})= N \left(\vec{a} \mid \hat{\vec{a}}, {H}^{-1}\right)
\end{align}
$$
[//]: # (\label{eq:gp_class_laplace_solution})

where $\hat{\vec{a}}$ is the mean
(mode) of the approximated function and the covariance
$H = W + C^{\top}$ with $W$ being a diagonal matrix with elements
$\sigma(a_{i})(1 - \sigma(a_i))$ while $a_n$ are the elements of
$\vec{a}$ and $C$ being the prior's covariance matrix. Then, the
Gaussian approximation of $p(a^{*} | t)$ is
$p\left(a^* \mid \vec{t}\right)  \approx {N}\left(a^* \mid \mu_{a^*}, \sigma_{a^*}^2\right)$
is 

$$
\begin{align} 
\mu_{a^*} &= \vec{k}^{*^T}(\vec{t}-\vec{\sigma}) \\
\sigma_{a^*}^2 &= k^{* *} - \vec{k}^{*^T}\left({W}^{-1} + C\right)^{-1} \vec{k}^*
\end{align}
$$
[//]: # (\label{eq:gp_class_final_solution})

where $\vec{\sigma}$ is the vector
that collects all the $\sigma(a_i))$ ; $\vec{k}^{*^T}$ all the values of
the kernel function evaluated with the new data point $\vec{x}^*$ and
the rest of the data $X$, i.e., $K (\vec{x}^*, X)$; $k^{* *}$ is the
kernel function evaluated at the point of interest, i.e.,
$K(\vec{x}^*, \vec{x}^*)$.

## References

-   short explanation here
    http://krasserm.github.io/2020/11/04/gaussian-processes-classification/

-   the rest is mainly from Bishop
    https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf
