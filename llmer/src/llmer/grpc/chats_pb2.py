# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: chats.proto
# Protobuf Python Version: 6.31.0
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import runtime_version as _runtime_version
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder

_runtime_version.ValidateProtobufRuntimeVersion(
    _runtime_version.Domain.PUBLIC, 6, 31, 0, "", "chats.proto"
)
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
    b'\n\x0b\x63hats.proto\x12\x0bllm_chat_v1"1\n\x13StartSessionRequest\x12\x0c\n\x04user\x18\x01 \x01(\t\x12\x0c\n\x04mode\x18\x02 \x01(\t""\n\x14StartSessionResponse\x12\n\n\x02id\x18\x01 \x01(\t"N\n\x08Question\x12\x12\n\nsession_id\x18\x01 \x01(\t\x12\x15\n\rsystem_prompt\x18\x02 \x01(\t\x12\x17\n\x0fquestion_prompt\x18\x03 \x01(\t"*\n\x06\x41nswer\x12\x12\n\nsession_id\x18\x01 \x01(\t\x12\x0c\n\x04text\x18\x02 \x01(\t2\xa0\x01\n\x0eLLMChatService\x12S\n\x0cStartSession\x12 .llm_chat_v1.StartSessionRequest\x1a!.llm_chat_v1.StartSessionResponse\x12\x39\n\x0bSendMessage\x12\x15.llm_chat_v1.Question\x1a\x13.llm_chat_v1.AnswerB5Z3../backend/grpc/gen/service/llm_chat/v1;llm_chat_v1b\x06proto3'
)

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, "chats_pb2", _globals)
if not _descriptor._USE_C_DESCRIPTORS:
    _globals["DESCRIPTOR"]._loaded_options = None
    _globals["DESCRIPTOR"]._serialized_options = (
        b"Z3../backend/grpc/gen/service/llm_chat/v1;llm_chat_v1"
    )
    _globals["_STARTSESSIONREQUEST"]._serialized_start = 28
    _globals["_STARTSESSIONREQUEST"]._serialized_end = 77
    _globals["_STARTSESSIONRESPONSE"]._serialized_start = 79
    _globals["_STARTSESSIONRESPONSE"]._serialized_end = 113
    _globals["_QUESTION"]._serialized_start = 115
    _globals["_QUESTION"]._serialized_end = 193
    _globals["_ANSWER"]._serialized_start = 195
    _globals["_ANSWER"]._serialized_end = 237
    _globals["_LLMCHATSERVICE"]._serialized_start = 240
    _globals["_LLMCHATSERVICE"]._serialized_end = 400
# @@protoc_insertion_point(module_scope)
